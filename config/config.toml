# Global LLM configuration (Default model)
[llm]
model = "claude-3-7-sonnet-20250219"
base_url = "https://api.anthropic.com/v1"
api_key = "${ANTHROPIC_API_KEY}" # Uses environment variable
max_tokens = 4096
temperature = 0.0
timeout = 180 # Set timeout to 3 minutes

# xAI Grok-2 configuration
[llm.grok]
model = "grok-2-latest"
api_key = "${XAI_API_KEY}" # Uses environment variable
max_tokens = 4096 
temperature = 0.0
timeout = 180 # Set timeout to 3 minutes

# Local LLM configuration
[llm.local]
model = "llama-3-70b"
base_url = "http://localhost:8000/v1"
api_key = "${LOCAL_API_KEY}" # Uses environment variable
max_tokens = 4096
temperature = 0.0
timeout = -1 # Set timeout to infinite

# [llm] #AZURE OPENAI:
# api_type= 'azure'
# model = "YOUR_MODEL_NAME" #"gpt-4o-mini"
# base_url = "{YOUR_AZURE_ENDPOINT.rstrip('/')}/openai/deployments/{AZURE_DEPOLYMENT_ID}"
# api_key = "AZURE API KEY"
# max_tokens = 8096
# temperature = 0.0
# api_version="AZURE API VERSION" #"2024-08-01-preview"
